{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CXo385nzCNI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dlib\n",
        "!pip install opencv-python\n",
        "!pip install scikit-image\n",
        "!pip install scikit-learn\n",
        "!pip install keras-vggface keras-applications"
      ],
      "metadata": {
        "id": "3-BN9FLR7MqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage.feature import hog\n",
        "from skimage import exposure\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from keras_vggface.utils import preprocess_input\n",
        "from keras.models import Model\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "GWFpIyb7605S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to dataset in Google Drive\n",
        "train_dataset_path = '/content/drive/My Drive/DataSet/train'\n",
        "test_dataset_path = '/content/drive/My Drive/DataSet/test'\n",
        "\n",
        "# List of expression folders (e.g., angry, happy, sad, etc.)\n",
        "expression_folders = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
      ],
      "metadata": {
        "id": "QZNE7AXs61k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Haar Cascade for face detection\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
      ],
      "metadata": {
        "id": "vP8wS1mV688_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):\n",
        "    # Load the image\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # Convert the image to grayscale (if not already grayscale)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the image\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    # If a face is detected, crop to the face, otherwise use the whole image\n",
        "    for (x, y, w, h) in faces:\n",
        "        face = gray[y:y+h, x:x+w]\n",
        "        break  # Only consider the first detected face\n",
        "\n",
        "    # Resize the face region to 48x48 (FER2013 image size)\n",
        "    face_resized = cv2.resize(face, (48, 48))\n",
        "\n",
        "    return face_resized"
      ],
      "metadata": {
        "id": "MyHg3nXg7XEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images(dataset_dir):\n",
        "    processed_images = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop through each expression folder\n",
        "    for expression in expression_folders:\n",
        "        expression_path = os.path.join(dataset_dir, expression)\n",
        "\n",
        "        # Process each image in the folder\n",
        "        for img_name in os.listdir(expression_path):\n",
        "            img_path = os.path.join(expression_path, img_name)\n",
        "            processed_img = preprocess_image(img_path)\n",
        "\n",
        "            # Append the processed image and label\n",
        "            processed_images.append(processed_img)\n",
        "            labels.append(expression)\n",
        "\n",
        "    return np.array(processed_images), np.array(labels)"
      ],
      "metadata": {
        "id": "udi4Y8HZ8Ynj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the train dataset\n",
        "train_images, train_labels = process_images(train_dataset_path)\n",
        "print(f'Processed {len(train_images)} train images.')\n",
        "\n",
        "# Process the test dataset\n",
        "test_images, test_labels = process_images(test_dataset_path)\n",
        "print(f'Processed {len(test_images)} test images.')"
      ],
      "metadata": {
        "id": "qLk5mo_39vjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a sample image\n",
        "plt.imshow(train_images[0], cmap='gray')\n",
        "plt.title(f'Label: {train_labels[0]}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5r_GKckC8b19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained face detector and shape predictor for facial landmarks\n",
        "face_detector = dlib.get_frontal_face_detector()\n",
        "landmark_predictor = dlib.shape_predictor('/content/drive/My Drive/shape_predictor_68_face_landmarks.dat')"
      ],
      "metadata": {
        "id": "isTAADRdAdZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_facial_landmarks(image):\n",
        "    # Convert the image to grayscale (if not already grayscale)\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the image using Dlib\n",
        "    faces = face_detector(gray)\n",
        "\n",
        "    for face in faces:\n",
        "        # Predict facial landmarks for the detected face\n",
        "        landmarks = landmark_predictor(gray, face)\n",
        "\n",
        "        # Convert the landmarks to a NumPy array of (x, y) coordinates\n",
        "        landmark_coords = np.zeros((68, 2), dtype=\"int\")\n",
        "        for i in range(0, 68):\n",
        "            landmark_coords[i] = (landmarks.part(i).x, landmarks.part(i).y)\n",
        "\n",
        "        # Return the landmark coordinates for the first detected face\n",
        "        return landmark_coords"
      ],
      "metadata": {
        "id": "tBEyjHJHAiQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_facial_landmarks(image, landmarks):\n",
        "    # Draw circles on the landmarks\n",
        "    for (x, y) in landmarks:\n",
        "        cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n",
        "\n",
        "    # Display the image with landmarks\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')  # Hide axis\n",
        "    plt.show()\n",
        "\n",
        "# Example usage on a sample image\n",
        "sample_image_path = '/content/drive/My Drive/DataSet/train/happy/image_1.jpg'  # Change this to your image path\n",
        "image = cv2.imread(sample_image_path)\n",
        "\n",
        "# Extract facial landmarks\n",
        "landmarks = extract_facial_landmarks(image)\n",
        "\n",
        "# Display image with facial landmarks\n",
        "if landmarks is not None:\n",
        "    display_facial_landmarks(image, landmarks)\n",
        "else:\n",
        "    print(\"No face detected in the image.\")"
      ],
      "metadata": {
        "id": "a2_wuqSqAlm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images_for_landmarks(dataset_dir):\n",
        "    image_landmarks = {}\n",
        "\n",
        "    # Loop through each expression folder\n",
        "    for expression in expression_folders:\n",
        "        expression_path = os.path.join(dataset_dir, expression)\n",
        "\n",
        "        # Process each image in the folder\n",
        "        for img_name in os.listdir(expression_path):\n",
        "            img_path = os.path.join(expression_path, img_name)\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            # Extract landmarks for the image\n",
        "            landmarks = extract_facial_landmarks(img)\n",
        "\n",
        "            if landmarks is not None:\n",
        "                # Store the landmarks\n",
        "                image_landmarks[img_name] = landmarks\n",
        "\n",
        "    return image_landmarks\n",
        "\n",
        "# Process the train dataset to extract landmarks\n",
        "train_image_landmarks = process_images_for_landmarks(train_dataset_path)"
      ],
      "metadata": {
        "id": "BShqWjqTAmtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_lbp_features(image, radius=1, n_points=8):\n",
        "    \"\"\"\n",
        "    Extract Local Binary Pattern (LBP) features from a grayscale image.\n",
        "\n",
        "    Parameters:\n",
        "    - image: Grayscale image.\n",
        "    - radius: Radius of the LBP.\n",
        "    - n_points: Number of points to sample around the pixel.\n",
        "\n",
        "    Returns:\n",
        "    - LBP image (encoded with LBP values).\n",
        "    \"\"\"\n",
        "    # Apply LBP\n",
        "    lbp = local_binary_pattern(image, n_points, radius, method=\"uniform\")\n",
        "\n",
        "    # Return the LBP image\n",
        "    return lbp"
      ],
      "metadata": {
        "id": "y7Y3-5PDApE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a sample grayscale image (preprocessed earlier)\n",
        "sample_image_path = '/content/drive/My Drive/DataSet/train/happy/image_1.jpg'  # Change this to your image path\n",
        "image = cv2.imread(sample_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Extract LBP features from the image\n",
        "lbp_image = extract_lbp_features(image)\n",
        "\n",
        "# Display the original image and the LBP image\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "ax1.imshow(image, cmap='gray')\n",
        "ax1.set_title('Original Image')\n",
        "ax1.axis('off')\n",
        "\n",
        "ax2.imshow(lbp_image, cmap='gray')\n",
        "ax2.set_title('LBP Image')\n",
        "ax2.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6W7haJ2vCiGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images_for_lbp(dataset_dir):\n",
        "    image_lbp_features = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop through each expression folder\n",
        "    for expression in expression_folders:\n",
        "        expression_path = os.path.join(dataset_dir, expression)\n",
        "\n",
        "        # Process each image in the folder\n",
        "        for img_name in os.listdir(expression_path):\n",
        "            img_path = os.path.join(expression_path, img_name)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load grayscale image\n",
        "\n",
        "            # Extract LBP features\n",
        "            lbp_features = extract_lbp_features(img)\n",
        "\n",
        "            # Store the LBP image and label\n",
        "            image_lbp_features.append(lbp_features)\n",
        "            labels.append(expression)\n",
        "\n",
        "    return np.array(image_lbp_features), np.array(labels)\n",
        "\n",
        "# Process the train dataset\n",
        "train_lbp_features, train_labels = process_images_for_lbp(train_dataset_path)\n",
        "print(f'Processed LBP features for {len(train_lbp_features)} train images.')\n",
        "\n",
        "# Process the test dataset\n",
        "test_lbp_features, test_labels = process_images_for_lbp(test_dataset_path)\n",
        "print(f'Processed LBP features for {len(test_lbp_features)} test images.')"
      ],
      "metadata": {
        "id": "-3IorIWOCoNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hog_features(image):\n",
        "    \"\"\"\n",
        "    Extract Histogram of Oriented Gradients (HOG) features from an image.\n",
        "\n",
        "    Parameters:\n",
        "    - image: Grayscale image.\n",
        "\n",
        "    Returns:\n",
        "    - HOG image (visualization) and HOG feature vector.\n",
        "    \"\"\"\n",
        "    # Compute HOG features and return the HOG image for visualization\n",
        "    hog_features, hog_image = hog(image, orientations=9, pixels_per_cell=(8, 8),\n",
        "                                  cells_per_block=(2, 2), block_norm='L2-Hys',\n",
        "                                  visualize=True, transform_sqrt=True)\n",
        "\n",
        "    return hog_features, hog_image"
      ],
      "metadata": {
        "id": "9Yl56LzGCsVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a sample grayscale image (preprocessed earlier)\n",
        "sample_image_path = '/content/drive/My Drive/DataSet/train/happy/image_1.jpg'  # Change this to your image path\n",
        "image = cv2.imread(sample_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Extract HOG features from the image\n",
        "hog_features, hog_image = extract_hog_features(image)\n",
        "\n",
        "# Display the original image and the HOG image\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "ax1.imshow(image, cmap='gray')\n",
        "ax1.set_title('Original Image')\n",
        "ax1.axis('off')\n",
        "\n",
        "ax2.imshow(hog_image, cmap='gray')\n",
        "ax2.set_title('HOG Image')\n",
        "ax2.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E3jqamrWD1Ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images_for_hog(dataset_dir):\n",
        "    image_hog_features = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop through each expression folder\n",
        "    for expression in expression_folders:\n",
        "        expression_path = os.path.join(dataset_dir, expression)\n",
        "\n",
        "        # Process each image in the folder\n",
        "        for img_name in os.listdir(expression_path):\n",
        "            img_path = os.path.join(expression_path, img_name)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load grayscale image\n",
        "\n",
        "            # Extract HOG features\n",
        "            hog_features, _ = extract_hog_features(img)\n",
        "\n",
        "            # Store the HOG feature vector and label\n",
        "            image_hog_features.append(hog_features)\n",
        "            labels.append(expression)\n",
        "\n",
        "    return np.array(image_hog_features), np.array(labels)\n",
        "\n",
        "# Process the train dataset\n",
        "train_hog_features, train_labels = process_images_for_hog(train_dataset_path)\n",
        "print(f'Processed HOG features for {len(train_hog_features)} train images.')\n",
        "\n",
        "# Process the test dataset\n",
        "test_hog_features, test_labels = process_images_for_hog(test_dataset_path)\n",
        "print(f'Processed HOG features for {len(test_hog_features)} test images.')"
      ],
      "metadata": {
        "id": "UDlAOzeGD4pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained VGGFace model\n",
        "base_model = VGGFace(model='vgg16', include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
        "\n",
        "# Print the model architecture to check layers\n",
        "base_model.summary()"
      ],
      "metadata": {
        "id": "v4x-MGfqD8BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image_for_vggface(image_path):\n",
        "    \"\"\"\n",
        "    Preprocess the image for VGGFace input.\n",
        "\n",
        "    Parameters:\n",
        "    - image_path: Path to the image to be processed.\n",
        "\n",
        "    Returns:\n",
        "    - Preprocessed image ready for VGGFace.\n",
        "    \"\"\"\n",
        "    # Load the image and resize to 224x224 (required input size for VGGFace)\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "\n",
        "    # Convert the image to a numpy array and expand dimensions to match VGGFace input\n",
        "    img_array = np.array(img, dtype=np.float32)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Preprocess the image (subtract mean pixel values)\n",
        "    img_array = preprocess_input(img_array, version=1)  # VGGFace version 1 preprocessing\n",
        "\n",
        "    return img_array"
      ],
      "metadata": {
        "id": "1h7LdIo9EM3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_vggface_features(image_path):\n",
        "    \"\"\"\n",
        "    Extract features from the pre-trained VGGFace model.\n",
        "\n",
        "    Parameters:\n",
        "    - image_path: Path to the image to be processed.\n",
        "\n",
        "    Returns:\n",
        "    - Feature vector extracted from the image.\n",
        "    \"\"\"\n",
        "    # Preprocess the image for VGGFace\n",
        "    preprocessed_image = preprocess_image_for_vggface(image_path)\n",
        "\n",
        "    # Extract features using VGGFace\n",
        "    features = base_model.predict(preprocessed_image)\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "SBbijMxcEULI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Extract features from a sample image\n",
        "sample_image_path = '/content/drive/My Drive/DataSet/train/happy/image_1.jpg'  # Change this to your image path\n",
        "vggface_features = extract_vggface_features(sample_image_path)\n",
        "\n",
        "print(\"VGGFace Features Shape:\", vggface_features.shape)"
      ],
      "metadata": {
        "id": "P-v0y8odEXRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images_for_vggface(dataset_dir):\n",
        "    image_vggface_features = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop through each expression folder\n",
        "    for expression in expression_folders:\n",
        "        expression_path = os.path.join(dataset_dir, expression)\n",
        "\n",
        "        # Process each image in the folder\n",
        "        for img_name in os.listdir(expression_path):\n",
        "            img_path = os.path.join(expression_path, img_name)\n",
        "\n",
        "            # Extract VGGFace features\n",
        "            vggface_features = extract_vggface_features(img_path)\n",
        "\n",
        "            # Store the feature vector and label\n",
        "            image_vggface_features.append(vggface_features)\n",
        "            labels.append(expression)\n",
        "\n",
        "    return np.array(image_vggface_features), np.array(labels)\n",
        "\n",
        "# Process the train dataset\n",
        "train_vggface_features, train_labels = process_images_for_vggface(train_dataset_path)\n",
        "print(f'Processed VGGFace features for {len(train_vggface_features)} train images.')\n",
        "\n",
        "# Process the test dataset\n",
        "test_vggface_features, test_labels = process_images_for_vggface(test_dataset_path)\n",
        "print(f'Processed VGGFace features for {len(test_vggface_features)} test images.')"
      ],
      "metadata": {
        "id": "cV71EBgHEZyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_pca(features, n_components=0.95):\n",
        "    \"\"\"\n",
        "    Apply PCA to reduce the dimensionality of the feature set.\n",
        "\n",
        "    Parameters:\n",
        "    - features: The input feature set (numpy array).\n",
        "    - n_components: Number of principal components to keep, or a float indicating the percentage of variance to retain.\n",
        "\n",
        "    Returns:\n",
        "    - Transformed feature set with reduced dimensions.\n",
        "    \"\"\"\n",
        "    # Initialize PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "\n",
        "    # Fit and transform the features\n",
        "    reduced_features = pca.fit_transform(features)\n",
        "\n",
        "    print(f\"Original number of features: {features.shape[1]}\")\n",
        "    print(f\"Reduced number of features: {reduced_features.shape[1]}\")\n",
        "\n",
        "    return reduced_features"
      ],
      "metadata": {
        "id": "fdkSlzdCEw31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume you have already extracted the HOG features for the train and test datasets\n",
        "# Example: HOG Features\n",
        "train_hog_features = np.array(train_hog_features)\n",
        "test_hog_features = np.array(test_hog_features)\n",
        "\n",
        "# Apply PCA to reduce dimensions of HOG features\n",
        "train_hog_pca = apply_pca(train_hog_features, n_components=0.95)\n",
        "test_hog_pca = apply_pca(test_hog_features, n_components=0.95)"
      ],
      "metadata": {
        "id": "MCROY0h7Ex74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For LBP Features\n",
        "train_lbp_pca = apply_pca(train_lbp_features, n_components=0.95)\n",
        "test_lbp_pca = apply_pca(test_lbp_features, n_components=0.95)\n",
        "\n",
        "# For VGGFace Features\n",
        "train_vggface_pca = apply_pca(train_vggface_features, n_components=0.95)\n",
        "test_vggface_pca = apply_pca(test_vggface_features, n_components=0.95)"
      ],
      "metadata": {
        "id": "uRCIyFktE0wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_pca_variance(features):\n",
        "    pca = PCA().fit(features)\n",
        "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Cumulative Explained Variance')\n",
        "    plt.title('PCA Variance Retention')\n",
        "    plt.show()\n",
        "\n",
        "# Example: Visualize variance explained by PCA on HOG features\n",
        "visualize_pca_variance(train_hog_features)"
      ],
      "metadata": {
        "id": "6RkVsn9-E3zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Using HOG PCA-reduced features for training SVM\n",
        "# Note: train_hog_pca and test_hog_pca are the reduced feature sets from the PCA step\n",
        "\n",
        "# Initialize the SVM model (RBF kernel is commonly used for non-linear classification)\n",
        "svm_model = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Train the SVM model on the training data\n",
        "svm_model.fit(train_hog_pca, train_labels)\n",
        "\n",
        "# Make predictions on the test data\n",
        "test_predictions = svm_model.predict(test_hog_pca)"
      ],
      "metadata": {
        "id": "OIlRxdt0E7Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(test_labels, test_predictions)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(test_labels, test_predictions)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "qKe2BAAfFSWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=expression_folders, yticklabels=expression_folders)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4OwRxIcdFU6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': [1, 0.1, 0.01],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2, cv=3)\n",
        "\n",
        "# Train the model with hyperparameter tuning\n",
        "grid.fit(train_hog_pca, train_labels)\n",
        "\n",
        "# Make predictions with the best estimator\n",
        "best_predictions = grid.predict(test_hog_pca)\n",
        "\n",
        "# Evaluate the best model\n",
        "best_accuracy = accuracy_score(test_labels, best_predictions)\n",
        "print(f'Best Accuracy: {best_accuracy * 100:.2f}%')\n",
        "print(\"Best Hyperparameters:\", grid.best_params_)"
      ],
      "metadata": {
        "id": "kvPkHWF2FZBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions from the trained SVM model\n",
        "# Example: Using predictions from HOG features\n",
        "# test_predictions = svm_model.predict(test_hog_pca)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(test_labels, test_predictions)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Compute precision, recall, and F1-score (for each class)\n",
        "precision = precision_score(test_labels, test_predictions, average='weighted')\n",
        "recall = recall_score(test_labels, test_predictions, average='weighted')\n",
        "f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
        "\n",
        "print(f'Precision: {precision * 100:.2f}%')\n",
        "print(f'Recall: {recall * 100:.2f}%')\n",
        "print(f'F1-Score: {f1 * 100:.2f}%')\n",
        "\n",
        "# Generate a full classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels, test_predictions, target_names=expression_folders))\n"
      ],
      "metadata": {
        "id": "oTKUjO4rFyKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
        "\n",
        "# Plot the confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=expression_folders, yticklabels=expression_folders)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KvtOreoxF-ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten raw image data for training\n",
        "def flatten_images(dataset_dir):\n",
        "    flattened_images = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop through each expression folder\n",
        "    for expression in expression_folders:\n",
        "        expression_path = os.path.join(dataset_dir, expression)\n",
        "\n",
        "        for img_name in os.listdir(expression_path):\n",
        "            img_path = os.path.join(expression_path, img_name)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            img_flattened = img.flatten()  # Flatten the image to a 1D array\n",
        "            flattened_images.append(img_flattened)\n",
        "            labels.append(expression)\n",
        "\n",
        "    return np.array(flattened_images), np.array(labels)\n",
        "\n",
        "# Process train and test data with raw pixel features\n",
        "train_raw_features, train_labels = flatten_images(train_dataset_path)\n",
        "test_raw_features, test_labels = flatten_images(test_dataset_path)"
      ],
      "metadata": {
        "id": "L63AclDSHB3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the SVM model with raw features\n",
        "svm_raw_model = SVC(kernel='rbf', random_state=42)\n",
        "svm_raw_model.fit(train_raw_features, train_labels)\n",
        "\n",
        "# Make predictions and evaluate performance\n",
        "test_raw_predictions = svm_raw_model.predict(test_raw_features)\n",
        "\n",
        "# Evaluate the baseline performance (accuracy, precision, recall, F1-score)\n",
        "print(\"Performance with Raw Features:\")\n",
        "print(f'Accuracy: {accuracy_score(test_labels, test_raw_predictions) * 100:.2f}%')\n",
        "print(classification_report(test_labels, test_raw_predictions))"
      ],
      "metadata": {
        "id": "va7QlOYKHC4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the performance of raw features vs. engineered features\n",
        "print(\"Performance with Engineered Features:\")\n",
        "print(f'Accuracy: {accuracy_score(test_labels, test_predictions) * 100:.2f}%')\n",
        "print(classification_report(test_labels, test_predictions))"
      ],
      "metadata": {
        "id": "xeFhSZcsHFsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_expression_on_test_data(test_dataset_path):\n",
        "    \"\"\"\n",
        "    This function processes the test images, extracts features using LBP, HOG, and VGGFace,\n",
        "    applies PCA for dimensionality reduction, and predicts facial expressions using the trained SVM model.\n",
        "\n",
        "    Parameters:\n",
        "    - test_dataset_path: Path to the test dataset.\n",
        "\n",
        "    Returns:\n",
        "    - test_predictions: Predicted emotion labels for the test dataset.\n",
        "    \"\"\"\n",
        "    test_predictions = []\n",
        "    test_labels = []\n",
        "\n",
        "    # Loop through each expression folder (e.g., happy, sad, etc.)\n",
        "    for expression in expression_folders:\n",
        "        expression_path = os.path.join(test_dataset_path, expression)\n",
        "\n",
        "        # Process each image in the folder\n",
        "        for img_name in os.listdir(expression_path):\n",
        "            img_path = os.path.join(expression_path, img_name)\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            # Step 1: Convert to grayscale and resize (for LBP, HOG)\n",
        "            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            img_gray_resized = cv2.resize(img_gray, (48, 48))  # For LBP, HOG\n",
        "\n",
        "            # Step 2: Extract features (LBP, HOG, VGGFace)\n",
        "            lbp_features = local_binary_pattern(img_gray_resized, P=8, R=1, method='uniform').flatten()\n",
        "            hog_features = hog(img_gray_resized, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), block_norm='L2-Hys', visualize=False)\n",
        "\n",
        "            # Resize and preprocess for VGGFace\n",
        "            img_resized = cv2.resize(img, (224, 224))\n",
        "            img_array = np.expand_dims(img_resized, axis=0).astype('float32')\n",
        "            img_preprocessed = preprocess_input(img_array, version=1)\n",
        "            vgg_features = base_model.predict(img_preprocessed).flatten()\n",
        "\n",
        "            # Step 3: Combine features\n",
        "            combined_features = np.hstack((lbp_features, hog_features, vgg_features))\n",
        "\n",
        "            # Step 4: Apply PCA for dimensionality reduction\n",
        "            reduced_features = pca_model.transform([combined_features])\n",
        "\n",
        "            # Step 5: Predict the emotion using the trained SVM model\n",
        "            predicted_emotion = svm_model.predict(reduced_features)\n",
        "\n",
        "            # Store the prediction and the true label\n",
        "            test_predictions.append(predicted_emotion[0])\n",
        "            test_labels.append(expression)  # The folder name represents the true label\n",
        "\n",
        "    return test_predictions, test_labels"
      ],
      "metadata": {
        "id": "R46QLJGxI8Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the facial expressions for all images in the test dataset\n",
        "test_predictions, test_labels = predict_expression_on_test_data(test_dataset_path)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(test_labels, test_predictions)\n",
        "print(f'Accuracy on Test Data: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the classification report for detailed metrics\n",
        "print(classification_report(test_labels, test_predictions, target_names=expression_folders))"
      ],
      "metadata": {
        "id": "FX2bVMZ5I9a_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}